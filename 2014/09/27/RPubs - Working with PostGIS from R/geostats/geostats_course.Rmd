Working with PostGIS from R
========================================================

# Introduction

Spatial data bases are widely used for storing and sharing institutional data. They are often run on a server in order to present spatial data to users on a network or over the internet, often using a mapserver as an intermediate interface. 

PostGIS works by adding a set of spatial operations to Postgresql. These spatial operators give the database the ability to run most of the common desktop GIS operations as SQL queries. So, you can also run PostGIS locally in order to augment both vector and raster processing in R. It is open source and  it is now easy to install under Windows or Linux. 

In order for a local install of PostGIS to be useful as a provider of additional functionality to R there must be a way to transfer data smoothly into and out of PostGIS from R and back.
One way to achieve this is to combine RODBC with rgdal. R is also available as a procedural language (PLR) that can be run directly within the database. It is worth looking at the potential of both approaches as additions to the toolbox of OSGEO functions available for R users. 
These examples have been run on Linux (Ubuntu). While all the code can be adapted to run on Windows quite easily, there are a couple of advantages of running PostGIS on Linux. The first is that it is easy to build from source under Linux. This is useful for raster processing as the code is still under active development and is constantly improving. The second is that it is simple to use system commands from R under linux in order to load data into PostGIS. This can also be achieved under Windows, but it is rather more complicated.

PostGIS can be set up very easily on Windows by adding the extension during the postgresql install procedure.

http://www.postgresql.org/download/windows/

If you want to follow the example of vector processing from Windows you may need to load layers using QGIS. The code for raster processing would not run "out of the box" on Windows, but could be adapted quite easily.

## Installing PostGIS under Ubuntu


Ubuntu is the most popular distribution for laptop users. PostGIS can be setup in just a few minutes by pasting in the lines below. Because there are rather a lot of different potential repositories for open source GIS packages it is possible to run into problems with dependencies if you have already installed GIS packages. You may need to check versions carefully if this does not work.
The following instructions were tested on tuesday 28 May 2013 on a fresh install of Ubuntu 12.04 Precise.
The simplest way of installing is simply to download the binary packages with their dependencies. 

### Installing from binaries

First install Postgresql. The default repository version at present is postgresql-9.1 (9.2 is now released). Use this version to ensure that all the instructions work.

```
sudo apt-get -y install postgresql postgresql-client postgresql-server-dev-9.1 pgadmin3
```
You will now have the database server running in the background.
Now comes a slightly confusing element. Postgresql is designed to store institutional data, so it has many built in security features. These are not needed for desktop use. In fact they can easily get in the way, particularly if you are relatively new to databases. So running a couple of lines at this point to give you superuser priviledges will prevent problems in the future. 
The default superuser is called 'postgres' After an install a password needs to be set for this user. The OS user also known as postgres also has been added (this is a cause of confusion). Set both these passwords to 'postgres' and also set up yourself up as a super user by  running these lines. 

```
sudo passwd postgres 
```
Type "postgres" after typing your own password.

Now alter the postgres user.

```
sudo -u postgres psql -c "alter user postgres with password 'postgres';"
sudo -u postgres createuser duncan --superuser
```

The program pgadmin provides a powerful graphical interface to postgresql. If you are new to databases there will be a lot to learn, but you may find many other uses for postgreslq in your research. It is an excelent open source replacement for MsAccess. 

Postgresql can now be used, as soon as you have your own PostGIS database. To download a recent binary version of postgis (plus qgis and grass if you don't already have them) run these lines.
```
sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable
sudo add-apt-repository ppa:sharpie/postgis-nightly
sudo apt-get update

sudo apt-get -y install postgresql-9.1-postgis qgis grass
sudo apt-get -y install odbc-postgresql unixodbc
sudo apt-get install postgresql-9.1-plr
```

That is it. We will setup the first database directly from R in order to show how closely the two can be linked, although this can also be done using pgadmin or the commandline.


### Building PostGIS from source

Because PostGIS raster is still under active development it is well worth installing the latest version of PostGIS if you want to use all the raster features. This is quite easy to do under Ubuntu. The following was also tested with a fresh install of Ubuntu 12.04 precise. Note that this can be run after installing the binaries without issue. In fact installing a binary package first is the easiest way to make sure that the loaders are available.

First load the necessary libraries.

```
sudo apt-get -y install proj.dev libgdal.dev
sudo apt-get install libgdal.dev libproj.dev libxml2-dev libgeos-dev 
sudo apt-get install libarmadillo-dev libpoppler-dev libepsilon-dev libexpat-dev liblzma-dev
```

Now bring down the latest source code, uncompress it, move into the directory and build. The default configuration includes raster functions.

```
wget http://postgis.net/stuff/postgis-2.1.0beta3dev.tar.gz
tar xvzf postgis-2.1*
cd postgis*
./configure
make
sudo make install
```
That should all compile and install using only the default configuration settings.

To build the latest PLR from source. This is probably not necessary if you downloaded the binary.

```
wget https://github.com/jconway/plr/archive/master.zip
unzip master.zip
 cd plr-master
 USE_PGXS=1 make
 sudo USE_PGXS=1 make install
```
## Downloading example data

The following file contains all the data needed to run the examples after setting up PostGIS.
https://dl.dropboxusercontent.com/u/2703650/Courses/geostats/geostats.zip


```
wget https://dl.dropboxusercontent.com/u/2703650/Courses/geostats/geostats.zip
unzip geostats.zip

```
# Using PostGIS from R

### Loading packages 

Two packages are used to link R with PostGIS using the method demonstrated here. RODBC provides Open Data Base Connectivity. This package can be used to connect R to any database, including MySQL and MSAccess.  In effect it allows R to "speak" SQL, providing an appropriate data base connection is established.

The other key package is rgdal. This is used to provide access to a wide range of spatial drivers, including PostGIS. Rgdal is needed in order to import data in shapefile or other formats. It is also useful for importing and exporting spatial objects to PostGIS as will be demonstrated using the fairly simple examples.



```{r}
setwd("/home/duncan/Dropbox/Public/Courses/geostats")
rm(list=ls())
library(RODBC)
library(rgdal)
```

### Creating a new data base

A Linux specific trick that can come in very handy is the ability to use system commands without dropping out of R. This can be useful for loading data into PostGIS.

For example, we can create a fresh PostGIS database to hold the example data directly from R.

```{r,eval=T}
system("createdb geostats")
```

This is the equivalent of running the following command in a terminal.

```
createdb geostats
```

Under Windows it would be preferable to create a new PostGIS database  using pgadmin. This is not shown here, but it is quite straight forward. A windows ODBC driver for Postgresql is also available and easy to install.

We now have a blank Postgresql database. However it is not yet a PostGIS data base.  

### Establishing an ODBC connection

So, let's set up a connection to this database and send some commands to it from R. We can use the first commands to create the PostGIS extension.

To do this in Ubuntu we need to edit the /etc/odbc.ini file.

```
sudo gedit /etc/odbc.ini
```
If your example database is called "geostats" just paste the following text into this file and save it. 

```
[geostats]
Driver = /usr/lib/i386-linux-gnu/odbc/psqlodbcw.so
Database = geostats
Servername = localhost
Username = postgres
Password = postgres
Protocol = 8.2.5
ReadOnly = 0
```
Now we can establish a connection to the database using the named dns.

```{r}
con <- odbcConnect("geostats")
```

Queries that do not return data can be run from R using odbcQuery.

```{r}
odbcQuery(con,"CREATE EXTENSION POSTGIS")
odbcQuery(con,"CREATE EXTENSION PLR")

```

If the queries return "1" they have been run successfully. Failed queries return -1. There is no way of getting diagnistic information in R. It is therefore a good idea to test more complex queries first using PgAdmin.

### Loading vector data into PostGIS from R

Now we can read in some example data. In my case I unzipped it to /home/duncan/geostats. If this is set as the working directory the uncommented line will work. Otherwise write your full path. I am afraid that you will have to rewrite the lines with paths several times in order to run the code, as absolute paths are needed for many operations so I have written in my own path rather than use many paste operations that might have appeared confusing.


```{r}
#states<-readOGR("/home/duncan/geostats/shapefiles","mex_states")
states<-readOGR("shapefiles","states")
plot(states)
box();axis(1);axis(2);grid()
```

Now write the shapefile to PostGIS

```{r}
writeOGR(states,"PG:dbname=geostats", layer_options="geometry_name=geom","states", "PostgreSQL")
```

This will automatically set up a spatial index, which is usually desirable for processing speed.  We will look at more explicit means of loading data later.


### Visualising results in QGIS

Quantum GIS was designed as a visualisation tool for PostGIS. The QGIS browser feature and the database manager make it very easy to move layers from PostGIS onto the canvas. First establish a connection with the database. 

![alt text](figure/sqgis1.png)

The following figure was produced by simply dragging the layer onto the canvas and dropping it on top of a Google image loaded using the Open Layers plugin.


![alt text](figure/sqgis2.png)

### Loading point data

Let's now read some point data from a text file.


```{r}
#towns<-read.csv("/home/duncan/geostats/textfiles/MexTowns.csv")
towns<-read.csv("textfiles/MexTowns.csv")
head(towns)
```

To turn this into a spatial object we just need to tell R which columns hold the coordinates and set the CRS. In this case it is the same as the shape file that we loaded (EPSG:4326)


```{r}
coordinates(towns)<-~x+y
proj4string(towns)<-proj4string(states)
writeOGR(towns,"PG:dbname=geostats", layer_options="geometry_name=geom","towns", "PostgreSQL")

```

Some more data on collections of Mexican Oak species.

```{r}
#oaks<-read.csv("/home/duncan/geostats/textfiles/MexOaks.csv")
oaks<-read.csv("textfiles/MexOaks.csv")
head(oaks)
coordinates(oaks)<-~x+y
proj4string(oaks)<-proj4string(states)

```

Now send the points to our database. However, the gdal driver runs very slowly when used directly from within R when there are many data records, as is the case for the oaks collection points. So this time we will try running the shapefile loader from the command line. 

```{r,message=FALSE,results='hide'}

writeOGR(oaks,"shapefiles", "oaks", "ESRI Shapefile")

#com<-"shp2pgsql -s 4326 -I /home/duncan/geostats/shapefiles/oaks.shp oaks| psql -d geostats;"
com<-"shp2pgsql -s 4326 -I shapefiles/oaks.shp oaks| psql -d geostats;"
system(com)

```

This line uses the shp2pgsql command. This is well documented and has many possible options including transformation from one CRS to another.

## Using SQL

Suppose we want to extract all the oak species in the State of Chiapas. This is easy to achieve using a spatial overlay in R.

```{r}

oaks$state<-over(oaks,states)$admin_name
chisoaks<-subset(oaks,state=="Chiapas")
chis<-subset(states,states@data$admin_name=="Chiapas")
plot(chis)
points(chisoaks,pch=21,bg=3,cex=0.4)
box();axis(1);axis(2);grid()
```

The same result can be achieved in PostGIS with a spatial query. 

```{r}
query<-"select genus,species,st_x(o.geom) x,st_y(o.geom) y from oaks o, states s where st_intersects(o.geom,s.geom) and admin_name like 'Chiapas'"
chisoaks<-sqlQuery(con,query)
coordinates(chisoaks)<-~x+y
plot(chis)
points(chisoaks,pch=21,bg=2,cex=0.4)
box();axis(1);axis(2);grid()
```

So there is little difference between the two approaches. Extracting the data using an overlay in R may be simpler and faster, but the sql may be more readable to some people. However there are operations that are not so straight forward in R.

## Buffering

A commmon operation is to find a buffer around points. This is particularly easy in PostGIS. However if you find the PostGIS st_buffer function you might first be tempted to try this in order to construct a 0.2 degree buffer (we will see how to use distance later) around the points.

```{r}
query<-"select gid,placename,st_buffer(geom,0.2) from towns"
buffer<-sqlQuery(con,query)

```

This would not work as a way of making a spatial object. The query would return a geometry column that R cannot understand directly.

```{r}
head(buffer)
```

A useful way around this is to use the gdal driver to pull in a view.

```{r}
query<-"create view temp_view as select gid,placename,st_buffer(geom,0.2) from towns"
odbcQuery(con,query)
buffer<-readOGR("PG:dbname=geostats", "temp_view")
query<-"drop view temp_view"
odbcQuery(con,query)
plot(states)
plot(buffer,add=T)
box();axis(1);axis(2);grid()
```

This works by creating a temporary view, which is then read into R using rgdal. You can think of a view as a query waiting to be run. In fact you could make a convenience function like this.

```{r}
getquery<-function(query){
 query<-paste("create view temp_view as ",query,sep="")
 odbcQuery(con,query)
 result<-readOGR("PG:dbname=geostats", "temp_view")
 odbcQuery(con,"drop view temp_view")
 return(result)
}

```

A nice feature of PostGIS is that geometries can be easily coerced to Geography in order to measure distances and areas. So this code will extract 10km buffers.


```{r}
buffer<-getquery("select gid,placename,st_buffer(geom::geography,10000)::geometry geom from towns")
plot(buffer)
box();axis(1);axis(2);grid()
```

The buffer could be used as an overlay within R if we were interested in finding all collections within a certain radius of a town. However a much better way to achieve this is to run a query that calculates distances directly.


```{r}
query<-"select o.gid,genus,species,placename, st_distance(o.geom::geography,t.geom::geography)/1000 distkm from towns t, oaks o where st_dwithin(t.geom,o.geom,0.1)"
d<-sqlQuery(con,query)
head(d)
```

This query shows several nice features of PostGIS. Instead of running a series of buffers we can use a single "st_dwithin" clause to narrow down the collections to those within a certain range of the towns. The range is in degrees (for speed). However by coercing the geometry to geography we measure the actual distance in meters. This sort of query is of course possible in R itself using spatial classes but it would be relatively tricky and would involve transforming the CRS in this case.

A slightly more complex query could be used to build a table for visualising the results by producing the shortest lines. This approach can also be used to produce networks. Interestingly PostGIS will measure the shortest edge to edge distance between polygons, which can be extremely useful for connectivity analysis and is particularly difficult to achieve in other ways. This code can be run in PGAdmin to produce a table for loading into QGIS.

```
create table oakstotown as
select o.gid,genus,species,placename, 
st_distance(o.geom::geography,t.geom::geography)/1000 distkm, st_shortestline(o.geom,t.geom) geom
 from towns t, oaks o where st_dwithin(t.geom,o.geom,0.1)
```

![alt text](figure/sdistance.png)

The above examples demonstrate that vector processing in PostGIS is quite straight forward. All the conventional GIS vector operations can be implemented directly in spatial SQL, which avoids the need to drop out of R to use a commercial vector GIS such as ArcGIS. Simple queries are quite intuitive and easy to follow, although complex queries can be more difficult to design.

## Raster data in PostGIS

Raster processing is a relatively new feature of PostGIS. There are many potential uses for this functionality for web based data processing. 
The benefits of using PostGIS raster as a tool for desktop data processing within R are not immediately clear, given the existing power and elegance of Robert Hijman's raster package. However there are some situations in which it comes into its own.

### Loading raster layers 

Raster data is usually loaded as tiles. Each tile is spatially indexed. This can allow PostGIS raster to process very large rasters. However there is some overhead involved in data access. Some operations may be rather slow as a result and are sensitive to tile size.


```{r}
command<-"raster2pgsql -s 4326 -d  -M -R /home/duncan/geostats/elevation.tif -F -t 100x100 elevation|psql -d geostats"
system(command)
```

This command loads 1km x 1km elevation data for Mexico (taken from Worldclim) into 100 x 100 pixel tiles. The command only takes a second or so to run as this is a relatively small raster layer. The -R flag means that the actual data stays outside the database and a reference is loaded. Rather suprisingly this speeds up processing of the data. The full path to the data must be used for this. Visualisation of the data in QGIS (or R) is best achieved using the original file. So the purpose of loading the data is to combine vector and raster operations using SQL statements.



![alt text](figure/sraster.png)

## Simple point overlay

If we wish to derive the elevation of point using a simple point on raster overlay we can run the following query.

```{r}
query<-"select o.gid,genus,species,st_value(rast,geom) from elevation, oaks o where st_intersects(geom,rast)"
d<-sqlQuery(con,query)
head(d)

```

Clearly this can also be achieved directly in in R

```{r}
library(raster)
#elev<-raster("/home/duncan/geostats/elevation.tif")
elev<-raster("elevation.tif")
library(lattice)
trellis.par.set(regions=list(col=terrain.colors(100)))
pts <- list("sp.points", oaks, pch = 4, col = "black", cex=0.2)
spplot(elev,sp.layout=list(pts))
oaks$elevation<-extract(elev,oaks)
head(oaks@data)

```

However, a common issue when overlaying point data is that coordinates of collection points are often not recorded precisely. So we may want to look at the range of elevations around a collection point in order to understand why some points may have unusual values.  We could do that by overlaying a buffer. This will return all the pixels falling within a certain distance of the point.

If the raster is held in small tiles, an overlay operation such as this will return results from several tiles at once. This does lead to rather complex data aggregation. 
A really nice feature of PostGIS is that R functions can be built within the data base using PLR. These can be used to help process results of queries "on the fly".


So we can build R functions within PostGIS using PLR that will take values of an array of pixels returned by a spatial overlay and return a result. So if we want to remove any NAs and return the range of values this function will work.

```
CREATE OR REPLACE FUNCTION range_elev (float[]) RETURNS float AS '
x<-arg1
x<-as.numeric(as.character(x)) 
x<-na.omit(x) 
max(x)-min(x)' 
LANGUAGE 'plr' STRICT;
```

```{r}
query<-"CREATE OR REPLACE FUNCTION range_elev (float[]) RETURNS float AS '
x<-arg1
x<-as.numeric(as.character(x)) 
x<-na.omit(x) 
max(x)-min(x)' 
LANGUAGE 'plr' STRICT;"
odbcQuery(con,query)

```

Now we can run the rather complex looking query below in order to obtain the range in elevation within a certain radius (in degrees) of the collections. Coercing the geometries to geography or transforming the CRS allows the query to measure distance in meters.

```{r}
query<-"select gid,genus,species,range_elev((st_dumpvalues(st_union(st_clip(rast,geom)))).valarray) from (select gid,species,genus, st_buffer(geom::geography,2000)::geometry geom from oaks limit 1400) o, elevation  where st_intersects(rast,geom) group by gid,genus,species;"
d<-sqlQuery(con,query)
head(d)
histogram(~range_elev|species,data=d,col="grey")
```

![alt text](figure/elev.png)


## Space time raster processing

PostGIS raster potentially comes into its own when dealing with long time series of large rasters. While processing speed can be an issue the underlying architecture is well designed to cope with such tasks. PostGIS is designed to be run on a server, so the task can be left to the most powerful processor. A simple trick for research groups on a tight budget is to install PostGIS on a PC with a solid state hard drive. 

One way of handling multitemporal raster data is to take advantage of table inheritance in Postgresql. 

This involves setting up a parent table that can be used directly in queries. At the same time data can be held in smaller tables that inherit the same properties. This makes PostGIS raster very scaleable. 

Here is a fairly simple example that uses two year's worth of monthly Modis NDVI data.


```{r}
path<-"/home/duncan/modis/"
query<-"create schema modis;"
odbcQuery(con,query)
query<-"drop table modis.ndvi cascade;"
odbcQuery(con,query)
query<-"CREATE TABLE modis.ndvi (
    id serial NOT NULL PRIMARY KEY,
    rid         int not null,
    fdate         date not null,
    rast        raster
);"
odbcQuery(con,query)

```

Setting up the subtables and loading the data is slightly tricky, but it can be achieved from R directly by using string operators to extract information from the file names.


```{r,message=FALSE,results='hide',warning=FALSE}
fls<-dir(path)
yrs<-substring(fls,1,4)
for(i in unique(yrs)){
  
  query<-paste("create table modis.ndvi_",i,"(CHECK (fdate >= DATE '",i,"-01-01' AND fdate <= DATE '",i,"-12-31' )) inherits (modis.ndvi);",sep="")
  print(query)
  odbcQuery(con,query)
  subfls<-fls[grep(i,fls)]
  for (f in subfls){
    f<-paste(path,f,sep="")
    command<-paste("raster2pgsql -s 4326 -d  -M -R ",f," -F -t 100x100 temp|psql -d geostats",sep="")
    print(command)
    system(command)
    query<-"alter table temp add column fdate date; update temp set fdate = substring(filename,1, 10)::date;"
    print(query)
    odbcQuery(con,query)
    query<-paste("insert into modis.ndvi_",i," (rid,fdate,rast) select rid,fdate,rast from temp;",sep="")
    print(query)
    odbcQuery(con,query)
  }
  query<-paste("CREATE INDEX modis_ndvi-",i,"_spindex ON modis.ndvi_",i," using gist(st_convexhull(rast));",sep="")
  print(query)
  odbcQuery(con,query)
}

```

Now that we have the data loaded we can use the multi-temporal table in much the same way as a table with only one layer. Again we can design an R function for data aggregation. In this case the NDVI data type is 16 bit signed integer, which has a theoretical range of values from -32,768 to +32,768. The documented data range is from -2000 to +10000. What that means in practice is that all negative values can be discounted as being produced by water or no data. Very low positive values (< 1000) are derived from urban areas or totally barren desert. So, to look at NDVI dynamics for vegetated areas we may want to impose a cutoff at around 1000 to ignore water and built up areas (taking the raw 16 bit values).



```{r}
query<-" CREATE OR REPLACE FUNCTION ndvi_mean (float[]) RETURNS float AS '
   x<-arg1
    x<-x[x>1000]
    x<-na.omit(x)
   mean(x)'
   LANGUAGE 'plr' STRICT;"

odbcQuery(con,query)
```



```{r}
query<-"select gid, extract(month from fdate) mon,extract(year from fdate) yr, ndvi_mean((st_dumpvalues(st_union(ST_Clip(rast,geom, true)))).valarray) from modis.ndvi, (select gid,st_buffer(geom::geography,10000)::geometry geom from oaks where species like 'sego%' limit 100) a where st_intersects(geom,rast) group by gid,fdate;"

d<-sqlQuery(con,query)
d$mon<-as.factor(d$mon)
d$yr<-as.factor(d$yr)
boxplot(ndvi_mean~mon,data=d,col="grey")


```

We can fit a mixed effects model in R to test the significance of any difference and extract confidence intervals.

```{r}
#library(lme4)
#mod1<-lme(ndvi_mean~yr+mon+(1|gid),data=d)
#mod2<-lmer(ndvi_mean~mon+(1|gid),data=d)
library(nlme)
mod1<-lme(ndvi_mean~yr+mon, random =~1|gid,data=d,method="ML")
mod2<-lme(ndvi_mean~mon,random =~1|gid,data=d,method="ML")
anova(mod1,mod2)
intervals(mod1)$fixed

```


Rainfall in southern Mexico was around 20% higher in 2004 than in 2003, which accounts for the increase in NDVI after taking into account the seasonal pattern. A more rigorous analysis would of course include spatial autocorrelation and would use a longer time series.


```{r,eval=T}
odbcClose(con)
system("dropdb geostats")

```
